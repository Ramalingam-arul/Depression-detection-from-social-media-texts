{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtaGUwwFO6fZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c88861-9675-4ad7-8861-3162af1706c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFFvAH7cPTwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92291433-4d4e-4d55-82e8-d0aafae202c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-2.0.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install -qqq ftfy\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "## for opening, manipulating, and saving many different image file f\n",
        "from PIL import Image\n",
        "\n",
        "import random \n",
        "\n",
        "## for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## for processing\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "import re\n",
        "import ftfy\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "\n",
        "#for expanding contracted words\n",
        "import sys  \n",
        "!{sys.executable} -m pip install contractions\n",
        "import contractions\n",
        "\n",
        "## WordCloud - Python linrary for creating image wordclouds\n",
        "from wordcloud import WordCloud\n",
        "from nltk import pos_tag ## For Parts of Speech tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SELECTING DATASETS"
      ],
      "metadata": {
        "id": "86cm8H5qEPJV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3A1JKIcqR4q1"
      },
      "outputs": [],
      "source": [
        "ds_all_train = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Dataset/depression_detection_Modified.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_all_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV-GFHj4XIlk",
        "outputId": "13fb6e75-380e-42cd-ce20-02bd7ca1f5ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39152, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train_dep = ds_all_train[ds_all_train['label']==1]"
      ],
      "metadata": {
        "id": "7DWVEdugWsOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train_notdep = ds_all_train[ds_all_train['label']==0]"
      ],
      "metadata": {
        "id": "ETXFQqPDWwL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_all = ds_train_dep.append(ds_train_notdep)"
      ],
      "metadata": {
        "id": "eM8gpIE-W3V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "AmRl-AooW6m8",
        "outputId": "e1a9d705-b7ce-44aa-c28d-be0fbd643899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text  label\n",
              "0      Open discussion. Between the Transfer Portal a...      1\n",
              "1      Plenty of things are changing in my life and t...      1\n",
              "2      I feel a little hopeless. Anyone else? #hopele...      1\n",
              "3      Which is more healthy? Hope, or hopelessness? ...      1\n",
              "4      So someone tell me how do I get over #HOPELESS...      1\n",
              "...                                                  ...    ...\n",
              "39143                             I am not in depression      0\n",
              "39144                                       I am not sad      0\n",
              "39147                      He is not depressed but happy      0\n",
              "39150                            He is not in depression      0\n",
              "39151                                       I am not ill      0\n",
              "\n",
              "[39152 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd9b30ef-19e0-4402-b001-c68e14e820b7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Open discussion. Between the Transfer Portal a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Plenty of things are changing in my life and t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I feel a little hopeless. Anyone else? #hopele...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Which is more healthy? Hope, or hopelessness? ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>So someone tell me how do I get over #HOPELESS...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39143</th>\n",
              "      <td>I am not in depression</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39144</th>\n",
              "      <td>I am not sad</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39147</th>\n",
              "      <td>He is not depressed but happy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39150</th>\n",
              "      <td>He is not in depression</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39151</th>\n",
              "      <td>I am not ill</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>39152 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd9b30ef-19e0-4402-b001-c68e14e820b7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dd9b30ef-19e0-4402-b001-c68e14e820b7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dd9b30ef-19e0-4402-b001-c68e14e820b7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET NOISE CLEANING (PRE-PROCESSING)"
      ],
      "metadata": {
        "id": "AM7ZMVn9Es9H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNDwEP1iUnl7"
      },
      "outputs": [],
      "source": [
        "def expandContracted(sentence):\n",
        "  expanded_words = []   \n",
        "  for word in sentence.split():\n",
        "    # using contractions.fix to expand the shortened words\n",
        "    expanded_words.append(contractions.fix(word))  \n",
        "  res = ' '.join(expanded_words)\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LEMMATIZATION WITH POS TAG\n",
        "# POS_TAGGER_FUNCTION : TYPE 1\n",
        "def pos_tagger(nltk_tag):\n",
        "\tif nltk_tag.startswith('J'):\n",
        "\t\treturn wordnet.ADJ\n",
        "\telif nltk_tag.startswith('V'):\n",
        "\t\treturn wordnet.VERB\n",
        "\telif nltk_tag.startswith('N'):\n",
        "\t\treturn wordnet.NOUN\n",
        "\telif nltk_tag.startswith('R'):\n",
        "\t\treturn wordnet.ADV\n",
        "\telse:\t\t\n",
        "\t\treturn None\n",
        "\n",
        "def lemmatize_func(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  # tokenize the sentence and find the POS tag for each token\n",
        "  pos_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "\n",
        "  wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
        "  lemmatized_sentence = []\n",
        "  for word, tag in wordnet_tagged:\n",
        "    if tag is None:\n",
        "      lemmatized_sentence.append(word)\n",
        "    else:\n",
        "      lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "  lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
        "\n",
        "  return lemmatized_sentence"
      ],
      "metadata": {
        "id": "8sv6JaGkQSIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Negation(sentence):\t\n",
        "  '''\n",
        "  Input: Tokenized sentence (List of words)\n",
        "  Output: Tokenized sentence with negation handled (List of words)\n",
        "  '''\n",
        "  temp = int(0)\n",
        "  sentence = nltk.word_tokenize(sentence)\n",
        "  for i in range(len(sentence)):\n",
        "      if sentence[i-1] in ['not',\"n't\"]:\n",
        "          antonyms = []\n",
        "          for syn in wordnet.synsets(sentence[i]):\n",
        "              syns = wordnet.synsets(sentence[i])\n",
        "              w1 = syns[0].name()\n",
        "              temp = 0\n",
        "              for l in syn.lemmas():\n",
        "                  if l.antonyms():\n",
        "                      antonyms.append(l.antonyms()[0].name())\n",
        "              max_dissimilarity = 0\n",
        "              for ant in antonyms:\n",
        "                  syns = wordnet.synsets(ant)\n",
        "                  w2 = syns[0].name()\n",
        "                  syns = wordnet.synsets(sentence[i])\n",
        "                  w1 = syns[0].name()\n",
        "                  word1 = wordnet.synset(w1)\n",
        "                  word2 = wordnet.synset(w2)\n",
        "                  if isinstance(word1.wup_similarity(word2), float) or isinstance(word1.wup_similarity(word2), int):\n",
        "                      temp = 1 - word1.wup_similarity(word2)\n",
        "                  if temp>max_dissimilarity:\n",
        "                      max_dissimilarity = temp\n",
        "                      antonym_max = ant\n",
        "                      sentence[i] = antonym_max\n",
        "                      sentence[i-1] = ''\n",
        "  res=\"\"\n",
        "  while '' in sentence:\n",
        "      sentence.remove('')\n",
        "    \n",
        "  for x in sentence:\n",
        "    res += x + \" \"\n",
        "  return res"
      ],
      "metadata": {
        "id": "JWHk_2s6_kT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGRkD_gnlHRA"
      },
      "outputs": [],
      "source": [
        "# PRE-PROCESSING FUNCTION\n",
        "def cleaning_process(input_tweets, input_labels):\n",
        "  cleaned_tweets = []\n",
        "  cleaned_labels = []\n",
        "  for (a_tweet, a_label) in zip(input_tweets, input_labels):\n",
        "\n",
        "    #convert to lowercase\n",
        "    a_tweet = a_tweet.lower()\n",
        "    \n",
        "    # if url links then don't append to avoid news articles\n",
        "    # also check tweet length, save those > 5 \n",
        "    if re.match(\"(\\w+:\\/\\/\\S+)\", a_tweet) == None and len(a_tweet) > 5:\n",
        "      #remove @mention\n",
        "      a_tweet = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", a_tweet)\n",
        "\n",
        "      #expand contraction\n",
        "      a_tweet = expandContracted(a_tweet)\n",
        "\n",
        "      a_tweet = Negation(a_tweet)\n",
        "\n",
        "      #remove punctuation\n",
        "      a_tweet = ' '.join(re.sub(\"([^0-9A-Za-z \\t])\", \" \", a_tweet).split())\n",
        "\n",
        "      #remove hashtag, @mention, HTML tags and image URLs\n",
        "      a_tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(\\#[A-Za-z0-9]+)|(<.>)|(pic\\.twitter\\.com\\/.*)\", \" \", a_tweet).split())\n",
        "      \n",
        "      #remove numbers\n",
        "      a_tweet = re.sub(r'\\d+', '', a_tweet)\n",
        "\n",
        "      #remove urls\n",
        "      a_tweet = re.sub(r'https?://\\S+|www\\.\\S+', '', a_tweet)\n",
        "\n",
        "      #fix weirdly encoded texts (Unicode correction)\n",
        "      a_tweet = ftfy.fix_text(a_tweet)\n",
        "\n",
        "      #stop words\n",
        "      stop_words = set(stopwords.words('english'))\n",
        "      word_tokens = nltk.word_tokenize(a_tweet)\n",
        "      a_tweet = [word for word in word_tokens if not word in stop_words or word == \"not\"]\n",
        "      a_tweet = ' '.join(a_tweet) # join words with a space in between them\n",
        "\n",
        "      #lemmatization\n",
        "      a_tweet = lemmatize_func(a_tweet)\n",
        "\n",
        "      cleaned_tweets.append(a_tweet)\n",
        "      cleaned_labels.append(a_label)\n",
        "  return (cleaned_tweets, cleaned_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLEANING OF EYE DATASET"
      ],
      "metadata": {
        "id": "xjf14HQfEy8k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8l3djVKmcEb"
      },
      "outputs": [],
      "source": [
        "ds_text_all_arr = [x for x in ds_all['text']]\n",
        "ds_label_all_arr = [x for x in ds_all['label']]\n",
        "(cleaned_ds_text_all_arr, cleaned_ds_labels_all_arr) = cleaning_process(ds_text_all_arr, ds_label_all_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTYS9UquoUfA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e47ed2-6370-4eb9-b7b1-1cecdfad17cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Open discussion. Between the Transfer Portal and the NIL, will the @NCAA become obsolete as an organization and governing body? @zlancaster91 @RAllenGoPokes #Hopelessness #GoPokes #LoyalandTrue', \"Plenty of things are changing in my life and the lives of those around me. There is one thing that doesn't change, my #hopelessness.\", 'I feel a little hopeless. Anyone else? #hopelessness', 'Which is more healthy? Hope, or hopelessness? #hope #Hopelessness #Mentalhealth', \"So someone tell me how do I get over #HOPELESSNESS? \\nI live in a world of #poverty surrounded by #PoorPeople.\\nPeople help us that are not much better off than we are.\\nIf not for my son's inability to care for himself I would likely give into the Hopelessness &amp; darkness.\", 'No parent deserves to experience the Indian legal system. #hopelessness', 'Being in a #union also looks a lot like being #alone.\\n\\nIt can feel like there’s no worse place to be sometimes.\\n\\n#hopelessness #unions #workersrights', 'I am so glad that @GreysABC is tackling the huge #healthcare professionals shortage we are facing and so saddened that we are not really living in their post pandemic world. \\n\\nThe #burnout, the #hopelessness, the #stress are palpable at every level and department of the system.', 'If you know someone who’s depressed please resolve to never ask them why. #depression  isn’t a straightforward response to a bad situation; depression just is, like the weather. Try to understand the blackness, lethargy, #hopelessness and loneliness they’re going through.', 'A #grateful #heart will #SeeGod. You will find in every #situation not #despair, #hopelessness, #annoyance, #frustration or #anger but a hidden #blessing, a #challenge to grow, an #anxiety to calm and a way to #serve. Go beyond mere #kindness and #love others!']\n"
          ]
        }
      ],
      "source": [
        "print(ds_text_all_arr[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XaPQ5VGnxpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86773c2a-e193-4ee1-aee6-28747a598096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['open discussion transfer portal nil become obsolete organization govern body hopelessness gopokes loyalandtrue', 'plenty thing change life life around one thing stay hopelessness', 'feel little hopeless anyone else hopelessness', 'healthy hope hopelessness hope hopelessness mentalhealth', 'someone tell get hopelessness live world poverty surround poorpeople people help us little well not son inability care would likely give hopelessness amp darkness', 'parent deserve experience indian legal system hopelessness', 'union also look lot like alone feel like bad place sometimes hopelessness union workersrights', 'glad tackle huge healthcare professional shortage face sadden not really live post pandemic world burnout hopelessness stress palpable every level department system', 'know someone depressed please resolve never ask depression not straightforward response bad situation depression like weather try understand blackness lethargy hopelessness loneliness go', 'grateful heart seegod find every situation not despair hopelessness annoyance frustration anger hidden blessing challenge grow anxiety calm way serve go beyond mere kindness love others']\n"
          ]
        }
      ],
      "source": [
        "print(cleaned_ds_text_all_arr[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BAG OF WORDS"
      ],
      "metadata": {
        "id": "fC28TImgk3UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from itertools import chain\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import bigrams\n",
        "\n",
        "def bow_for_line(corpus, top_words):\n",
        "  bag_of_words = []\n",
        "  for doc in corpus:\n",
        "    unigram_words = doc\n",
        "    bigram_words = list(bigrams(doc))\n",
        "    trigram_words = list(ngrams(doc, 3))\n",
        "    \n",
        "    word_counts = Counter(doc)\n",
        "    bigram_word_counts = Counter(bigram_words)\n",
        "    trigram_word_counts = Counter(trigram_words)\n",
        "\n",
        "    word_count=dict(word_counts)\n",
        "    word_count.update(dict(bigram_word_counts))\n",
        "    word_count.update(dict(trigram_word_counts))\n",
        "\n",
        "    dict_words = dict(word_count)\n",
        "    line_words = Counter(dict_words)\n",
        "\n",
        "    row = [line_words[word] if word in line_words else 0 for word in top_words]\n",
        "    bag_of_words.append(row)\n",
        "\n",
        "  return bag_of_words\n",
        "\n",
        "def bow(data_train):\n",
        "  # Preprocess and tokenize the corpus\n",
        "  corpus = [nltk.word_tokenize(doc.lower()) for doc in data_train]\n",
        "\n",
        "  # Calculate the frequency of each word in the corpus\n",
        "  word_counts = Counter(chain.from_iterable(corpus))\n",
        "\n",
        "  bigram_words = []\n",
        "  for doc in corpus:\n",
        "    words = list(bigrams(doc))\n",
        "    bigram_words.append(words)\n",
        "\n",
        "  # Calculate the frequency of each bi and trigram words in the corpus\n",
        "  bigram_word_counts = Counter(chain.from_iterable(bigram_words))\n",
        "\n",
        "  trigram_words = []\n",
        "  for doc in corpus:\n",
        "    words = list(ngrams(doc, 3))\n",
        "    trigram_words.append(words)\n",
        "\n",
        "  trigram_word_counts = Counter(chain.from_iterable(trigram_words))\n",
        "\n",
        "  word_count=dict(word_counts)\n",
        "  word_count.update(dict(bigram_word_counts))\n",
        "  word_count.update(dict(trigram_word_counts))\n",
        "\n",
        "  word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
        "  dict_words = dict(word_count)\n",
        "  final_words = Counter(dict_words)\n",
        "\n",
        "  # # Select the top k most frequent words as columns\n",
        "  top_words = [word for word, count in final_words.most_common(5000)]\n",
        "\n",
        "  # # # Create the bag of words matrix\n",
        "  bag_of_words = bow_for_line(corpus, top_words)\n",
        "  \n",
        "  return bag_of_words, top_words"
      ],
      "metadata": {
        "id": "uRDZDIabhKBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOW for EYE"
      ],
      "metadata": {
        "id": "DfjM82NA9-nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words, top_words = bow(cleaned_ds_text_all_arr)\n",
        "\n",
        "print(len(bag_of_words[0]))\n",
        "print(len(top_words))"
      ],
      "metadata": {
        "id": "DfSiW00ghvfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bed16a6-43f2-4db4-ad43-957f3cb8590a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_words = []\n",
        "for word in top_words:\n",
        "  if isinstance(word, tuple):\n",
        "    column_words.append(\" \".join(word))\n",
        "  else:\n",
        "    column_words.append(word)"
      ],
      "metadata": {
        "id": "Th5w-tRh6UbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame (bag_of_words, columns = column_words)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "EhW9brQZk8z_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "f67e4fff-593d-406b-bba0-5e2caa0c2633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   mentalhealth  anxiety  not  depression  quot  sad  get  go  love  like  \\\n",
              "0             0        0    0           0     0    0    0   0     0     0   \n",
              "1             0        0    0           0     0    0    0   0     0     0   \n",
              "2             0        0    0           0     0    0    0   0     0     0   \n",
              "3             1        0    0           0     0    0    0   0     0     0   \n",
              "4             0        0    1           0     0    0    1   0     0     0   \n",
              "\n",
              "   ...  amp tweet  time quot  quot god  haha love  one quot  \\\n",
              "0  ...          0          0         0          0         0   \n",
              "1  ...          0          0         0          0         0   \n",
              "2  ...          0          0         0          0         0   \n",
              "3  ...          0          0         0          0         0   \n",
              "4  ...          0          0         0          0         0   \n",
              "\n",
              "   followfriday great  lol well  shaundiviney shaundiviney  quot really  \\\n",
              "0                   0         0                          0            0   \n",
              "1                   0         0                          0            0   \n",
              "2                   0         0                          0            0   \n",
              "3                   0         0                          0            0   \n",
              "4                   0         0                          0            0   \n",
              "\n",
              "   thanks followfriday  \n",
              "0                    0  \n",
              "1                    0  \n",
              "2                    0  \n",
              "3                    0  \n",
              "4                    0  \n",
              "\n",
              "[5 rows x 5000 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cdebeea0-bddd-483e-8e44-56edcfd6ae69\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mentalhealth</th>\n",
              "      <th>anxiety</th>\n",
              "      <th>not</th>\n",
              "      <th>depression</th>\n",
              "      <th>quot</th>\n",
              "      <th>sad</th>\n",
              "      <th>get</th>\n",
              "      <th>go</th>\n",
              "      <th>love</th>\n",
              "      <th>like</th>\n",
              "      <th>...</th>\n",
              "      <th>amp tweet</th>\n",
              "      <th>time quot</th>\n",
              "      <th>quot god</th>\n",
              "      <th>haha love</th>\n",
              "      <th>one quot</th>\n",
              "      <th>followfriday great</th>\n",
              "      <th>lol well</th>\n",
              "      <th>shaundiviney shaundiviney</th>\n",
              "      <th>quot really</th>\n",
              "      <th>thanks followfriday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 5000 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cdebeea0-bddd-483e-8e44-56edcfd6ae69')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cdebeea0-bddd-483e-8e44-56edcfd6ae69 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cdebeea0-bddd-483e-8e44-56edcfd6ae69');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df\n",
        "y = cleaned_ds_labels_all_arr\n",
        "print(len(X), len(y))"
      ],
      "metadata": {
        "id": "eMto_2J-Td6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d803dfb-05fe-405e-f6a7-875acd6bceb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39152 39152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 - FOLD CROSS VALIDATION FOR MODELS"
      ],
      "metadata": {
        "id": "XZbhAzpfl8C7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "splits=list(kf.split(X,y))"
      ],
      "metadata": {
        "id": "vG7tqtoy_2b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "waprlXpzxoPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LogReg_scores_eye = []\n",
        "LogReg_scores = []\n",
        "\n",
        "f1_scores_eye = []\n",
        "precision_scores_eye = []\n",
        "recall_scores_eye = []\n",
        "\n",
        "for k in range(10):\n",
        "  train_indices, test_indices = splits[k]\n",
        "  X_train = np.array(X)[train_indices.astype(int)]\n",
        "  X_test = np.array(X)[test_indices.astype(int)]\n",
        "\n",
        "  y_train = np.array(y)[train_indices.astype(int)]\n",
        "  y_test = np.array(y)[test_indices.astype(int)]\n",
        "\n",
        "  model_Log = LogisticRegression()\n",
        "  model_Log.fit(X_train, y_train)\n",
        "  y_pred = model_Log.predict(X_test)\n",
        "  \n",
        "  print(\"Fold - {} completed\".format(k))\n",
        "\n",
        "  #Eye testing\n",
        "  LogReg_scores_eye.append(model_Log.score(X_test, y_test))\n",
        "  y_pred_eye = model_Log.predict(X_test)\n",
        "  f1_scores_eye.append(f1_score(y_test, y_pred_eye, average='macro'))\n",
        "  precision_scores_eye.append(precision_score(y_test, y_pred_eye, average='macro'))\n",
        "  recall_scores_eye.append(recall_score(y_test, y_pred_eye, average='macro'))\n",
        "\n",
        "print('\\n\\n10 Fold Cross-Validation accuracy for training and testing with EYE dataset:')\n",
        "print('CV accuracy LogReg: %.4f' %(np.mean(LogReg_scores_eye)*100 + np.std(LogReg_scores_eye)*100))\n",
        "print('CV F1 score: %.4f' %(np.mean(f1_scores_eye)*100 + np.std(f1_scores_eye)*100))\n",
        "print('CV Precision: %.4f' %(np.mean(precision_scores_eye)*100 + np.std(precision_scores_eye)*100))\n",
        "print('CV Recall: %.4f' %(np.mean(recall_scores_eye)*100 + np.std(recall_scores_eye)*100))\n",
        "\n",
        "LogReg_scores.append(np.mean(LogReg_scores_eye)*100 + np.std(LogReg_scores_eye)*100)\n",
        "LogReg_scores.append(np.mean(precision_scores_eye)*100 + np.std(precision_scores_eye)*100)\n",
        "LogReg_scores.append(np.mean(recall_scores_eye)*100 + np.std(recall_scores_eye)*100)\n",
        "LogReg_scores.append(np.mean(f1_scores_eye)*100 + np.std(f1_scores_eye)*100)"
      ],
      "metadata": {
        "id": "UErFle841srW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc77e5b-8be4-4ea7-e58c-33cba82cc8fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold - 0 completed\n",
            "Fold - 1 completed\n",
            "Fold - 2 completed\n",
            "Fold - 3 completed\n",
            "Fold - 4 completed\n",
            "Fold - 5 completed\n",
            "Fold - 6 completed\n",
            "Fold - 7 completed\n",
            "Fold - 8 completed\n",
            "Fold - 9 completed\n",
            "\n",
            "\n",
            "10 Fold Cross-Validation accuracy for training and testing with EYE dataset:\n",
            "CV accuracy LogReg: 99.9043\n",
            "CV F1 score: 99.9038\n",
            "CV Precision: 99.9004\n",
            "CV Recall: 99.9075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NO CV MODEL"
      ],
      "metadata": {
        "id": "krl9k4mRe37F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "OWqGXS2xfYv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
      ],
      "metadata": {
        "id": "H9psnF6VfMAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model_Log.predict(X_test)"
      ],
      "metadata": {
        "id": "ebPX4mHmfGNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "9qZZGAszfhcH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9747a539-d734-4ff0-fda2-d8db7dac062c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9988390991409334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAVE MODEL"
      ],
      "metadata": {
        "id": "lzLjAV3Rf8DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "M-z1wpZme97s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'Final_Model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "st8EVeUjetwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAVE TOP WORDS"
      ],
      "metadata": {
        "id": "skQlpM8uggc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(r'top_words.txt', 'w') as fp:\n",
        "    fp.write(\"\\n\".join(str(item) for item in top_words))"
      ],
      "metadata": {
        "id": "h13pcEDqgH-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4GmqpNGtiK25"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}